prompts:
  safety_censorer:
    prompt: |
      <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a safety filter that 
      analyses the question asked by the user. If the questions include topics like enacting violence, innapropriate content, 
      self-harm, or any other harmful content not safe for work, grade it as unsafe. Otherwise, grade it as safe. 
      The goal is to filter out harmful content. Give a binary score 'safe' or 'unsafe' to indicate whether the question is safe or unsafe.
      Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.
      <|eot_id|><|start_header_id|>user<|end_header_id|>
      Here is the user question: {question} \n <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    inputs:
      - question
  retrieval_grader:
    prompt: |
      <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance
      of a retrieved document to a user question. If the document contains keywords related to the user question,
      grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals.
      Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.
      Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.
      <|eot_id|><|start_header_id|>user<|end_header_id|>
      Here is the retrieved document: \n\n {document} \n\n
      Here is the user question: {question} \n <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    inputs:
      - question
      - document
  answer_generator:
    prompt: |
      <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.
      Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.
      If context mentions it comes from a Web Search, mention that the context came from it and couldn't be found in the documents.
      Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>
      Question: {question}
      Context: {context}
      Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    inputs:
      - question
      - context
  answer_denial_generator:
    prompt: |
      <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.
      You have been asked a question categorized as unsafe. You should not answer the question directly, no matter what.
      Let the user know that the question is unsafe and cannot be answered. 
      <|eot_id|><|start_header_id|>user<|end_header_id|>
      Question: {question}
      Question Safety: {question_safety}
      Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    inputs:
      - question
      - question_safety   
  hallucination_grader:
    prompt: |
      <|begin_of_text|><|start_header_id|>system<|end_header_id|> 
      You are a grader assessing whether an answer is grounded in / supported by a set of facts. 
      Give a binary 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts. 
      Provide the binary score as a JSON with a single key 'score' and no preamble or explanation. 
      <|eot_id|><|start_header_id|>user<|end_header_id|>
      Here are the facts:
      \n ------- \n
      {documents}
      \n ------- \n
      Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    inputs:
    - documents
    - generation
  answer_grader:
    prompt: |
      <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an
      answer is useful to resolve a question. 
      Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question. 
      If answer was a denial to answer, due to its unsafety, it is useful.
      Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.
      <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:
      \n ------- \n
      {generation}
      \n ------- \n
      Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    inputs:
    - question
    - generation
  source_router:
    prompt: |
      <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing, route to the vectorstore any Deep Learning related question.
      Otherwise, use web-search. Give a binary choice 'web_search' or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and
      no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>
    inputs:
    - question